Looking at the Kibana codebase structure, I can see a complex monorepo with packages distributed across multiple directories and plugins organized by solution areas. Here are 3 comprehensive approaches to analyze README documentation quality:

## ✅ IMPLEMENTATION COMPLETE

All three approaches have been implemented as executable scripts in the `/scripts` directory:

### Quick Start
```bash
# Run bootstrap first (one-time setup)
yarn kbn bootstrap

# Option 1: Use the interactive runner
./scripts/run_doc_analysis.sh

# Option 2: Run scripts directly
node scripts/analyze_documentation.js       # Basic analysis
./scripts/doc_analysis.sh                   # Shell analysis  
node scripts/advanced_doc_analysis.js       # Advanced analysis
```

See `/scripts/DOCUMENTATION_ANALYSIS.md` for detailed documentation.

---

## Approach 1: Script-Based Analysis with Quality Scoring

✅ **Implemented as:** `scripts/analyze_documentation.js`

Create a Node.js script that traverses the codebase and scores README quality:

```typescript
import fs from 'fs';
import path from 'path';
import { glob } from 'glob';

interface DocumentationStats {
  packagePath: string;
  hasReadme: boolean;
  isGenerated: boolean;
  isPlaceholder: boolean;
  qualityScore: number;
  wordCount: number;
  sections: string[];
}

// Define patterns that indicate generated/placeholder content
const GENERATED_PATTERNS = [
  /This is a placeholder README/i,
  /TODO: Add documentation/i,
  /Empty package generated by @kbn\/plugin-generator/i,
  /This plugin was generated/i,
  /Add your plugin description here/i
];

const QUALITY_INDICATORS = {
  installation: /installation|install|setup/i,
  usage: /usage|example|how to/i,
  api: /api|interface|methods/i,
  testing: /test|testing|spec/i,
  contributing: /contribut|develop/i,
  architecture: /architecture|design|structure/i
};

function analyzeReadme(content: string): Partial<DocumentationStats> {
  const wordCount = content.split(/\s+/).length;
  const isGenerated = GENERATED_PATTERNS.some(pattern => pattern.test(content));
  const isPlaceholder = wordCount < 50 || isGenerated;
  
  // Extract sections (lines starting with #)
  const sections = content.match(/^#+\s+(.+)$/gm) || [];
  
  // Calculate quality score
  let qualityScore = 0;
  if (wordCount > 100) qualityScore += 2;
  if (wordCount > 300) qualityScore += 3;
  
  Object.values(QUALITY_INDICATORS).forEach(pattern => {
    if (pattern.test(content)) qualityScore += 1;
  });
  
  if (sections.length > 3) qualityScore += 2;
  
  return {
    isGenerated,
    isPlaceholder,
    qualityScore: Math.min(qualityScore, 10),
    wordCount,
    sections: sections.map(s => s.replace(/^#+\s+/, ''))
  };
}

async function analyzeDocumentation() {
  const packageDirs = [
    'packages/**',
    'src/platform/**',
    'x-pack/packages/**',
    'x-pack/platform/packages/**',
    'x-pack/solutions/*/packages/**',
    'x-pack/solutions/*/plugins/**'
  ];
  
  const results: DocumentationStats[] = [];
  
  for (const pattern of packageDirs) {
    const paths = await glob(pattern, { onlyDirectories: true });
    
    for (const pkgPath of paths) {
      const readmePath = path.join(pkgPath, 'README.md');
      const hasReadme = fs.existsSync(readmePath);
      
      let analysis: Partial<DocumentationStats> = {
        hasReadme: false,
        isGenerated: false,
        isPlaceholder: true,
        qualityScore: 0,
        wordCount: 0,
        sections: []
      };
      
      if (hasReadme) {
        const content = fs.readFileSync(readmePath, 'utf-8');
        analysis = { ...analysis, hasReadme: true, ...analyzeReadme(content) };
      }
      
      results.push({
        packagePath: pkgPath,
        ...analysis
      } as DocumentationStats);
    }
  }
  
  return results;
}

// Generate comprehensive report
analyzeDocumentation().then(results => {
  const stats = {
    total: results.length,
    noReadme: results.filter(r => !r.hasReadme).length,
    hasReadme: results.filter(r => r.hasReadme).length,
    generated: results.filter(r => r.isGenerated).length,
    placeholder: results.filter(r => r.isPlaceholder).length,
    lowQuality: results.filter(r => r.qualityScore <= 3).length,
    mediumQuality: results.filter(r => r.qualityScore > 3 && r.qualityScore <= 6).length,
    highQuality: results.filter(r => r.qualityScore > 6).length
  };
  
  console.log('Kibana Documentation Analysis:');
  console.log(`Total packages/plugins: ${stats.total}`);
  console.log(`No README: ${stats.noReadme} (${(stats.noReadme/stats.total*100).toFixed(1)}%)`);
  console.log(`Generated/Placeholder: ${stats.placeholder} (${(stats.placeholder/stats.total*100).toFixed(1)}%)`);
  console.log(`Low Quality (≤3): ${stats.lowQuality} (${(stats.lowQuality/stats.total*100).toFixed(1)}%)`);
  console.log(`Medium Quality (4-6): ${stats.mediumQuality} (${(stats.mediumQuality/stats.total*100).toFixed(1)}%)`);
  console.log(`High Quality (>6): ${stats.highQuality} (${(stats.highQuality/stats.total*100).toFixed(1)}%)`);
});
```

## Approach 2: Shell Script with Pattern Matching

✅ **Implemented as:** `scripts/doc_analysis.sh`

A faster shell-based approach using find and grep:

```bash
#!/bin/bash
# filepath: scripts/doc_analysis.sh

echo "=== Kibana Documentation Analysis ==="

# Define search paths for packages and plugins
SEARCH_PATHS=(
  "packages"
  "src/platform"
  "x-pack/packages"
  "x-pack/platform/packages"
  "x-pack/solutions/*/packages"
  "x-pack/solutions/*/plugins"
)

# Count total packages/plugins
total_dirs=0
for path in "${SEARCH_PATHS[@]}"; do
  if [[ "$path" == *"*"* ]]; then
    # Handle glob patterns
    total_dirs=$((total_dirs + $(find . -type d -path "./$path" | wc -l)))
  else
    total_dirs=$((total_dirs + $(find "$path" -mindepth 1 -maxdepth 1 -type d 2>/dev/null | wc -l)))
  fi
done

echo "Total packages/plugins found: $total_dirs"

# Find directories without README.md
echo -n "Packages without README.md: "
no_readme=0
for path in "${SEARCH_PATHS[@]}"; do
  if [[ "$path" == *"*"* ]]; then
    dirs=$(find . -type d -path "./$path" 2>/dev/null)
  else
    dirs=$(find "$path" -mindepth 1 -maxdepth 1 -type d 2>/dev/null)
  fi
  
  for dir in $dirs; do
    if [[ ! -f "$dir/README.md" ]]; then
      ((no_readme++))
    fi
  done
done
echo "$no_readme ($(echo "scale=1; $no_readme * 100 / $total_dirs" | bc)%)"

# Find generated/placeholder READMEs
echo -n "Generated/Placeholder READMEs: "
generated=$(find . -name "README.md" -path "./packages/*" -o -path "./src/platform/*" -o -path "./x-pack/packages/*" -o -path "./x-pack/platform/packages/*" -o -path "./x-pack/solutions/*/packages/*" -o -path "./x-pack/solutions/*/plugins/*" | xargs grep -l -E "(This is a placeholder|TODO: Add documentation|Generated by.*generator|Add your plugin description)" 2>/dev/null | wc -l)
echo "$generated"

# Find short READMEs (likely minimal)
echo -n "Short READMEs (<100 words): "
short_readmes=0
for path in "${SEARCH_PATHS[@]}"; do
  if [[ "$path" == *"*"* ]]; then
    readmes=$(find . -name "README.md" -path "./$path" 2>/dev/null)
  else
    readmes=$(find "$path" -name "README.md" 2>/dev/null)
  fi
  
  for readme in $readmes; do
    word_count=$(wc -w < "$readme" 2>/dev/null || echo 0)
    if [[ $word_count -lt 100 ]]; then
      ((short_readmes++))
    fi
  done
done
echo "$short_readmes"

# Find comprehensive READMEs (with multiple sections)
echo -n "Comprehensive READMEs (>300 words + sections): "
comprehensive=0
for path in "${SEARCH_PATHS[@]}"; do
  if [[ "$path" == *"*"* ]]; then
    readmes=$(find . -name "README.md" -path "./$path" 2>/dev/null)
  else
    readmes=$(find "$path" -name "README.md" 2>/dev/null)
  fi
  
  for readme in $readmes; do
    word_count=$(wc -w < "$readme" 2>/dev/null || echo 0)
    section_count=$(grep -c "^#" "$readme" 2>/dev/null || echo 0)
    if [[ $word_count -gt 300 && $section_count -gt 3 ]]; then
      ((comprehensive++))
    fi
  done
done
echo "$comprehensive"

echo ""
echo "=== Quality Breakdown ==="
echo "No Documentation: $no_readme"
echo "Generated/Placeholder: $generated" 
echo "Minimal Documentation: $short_readmes"
echo "Comprehensive Documentation: $comprehensive"
```

## Approach 3: AST-Based Analysis with Content Categorization

✅ **Implemented as:** `scripts/advanced_doc_analysis.js`

A more sophisticated approach using markdown parsing:

```typescript
import fs from 'fs';
import path from 'path';
import { glob } from 'glob';
import { unified } from 'unified';
import remarkParse from 'remark-parse';
import { visit } from 'unist-util-visit';

interface DetailedAnalysis {
  packagePath: string;
  packageType: 'core' | 'platform' | 'solution' | 'x-pack';
  hasReadme: boolean;
  analysis: {
    wordCount: number;
    headingCount: number;
    codeBlockCount: number;
    linkCount: number;
    hasInstallation: boolean;
    hasUsage: boolean;
    hasApi: boolean;
    hasTesting: boolean;
    hasContributing: boolean;
    isGenerated: boolean;
    qualityTier: 'none' | 'placeholder' | 'basic' | 'good' | 'excellent';
  };
}

function categorizePackage(pkgPath: string): string {
  if (pkgPath.startsWith('src/')) return 'core';
  if (pkgPath.includes('platform/packages')) return 'platform';
  if (pkgPath.includes('solutions/')) return 'solution';
  if (pkgPath.startsWith('x-pack/')) return 'x-pack';
  return 'other';
}

function analyzeMarkdown(content: string): DetailedAnalysis['analysis'] {
  const processor = unified().use(remarkParse);
  const tree = processor.parse(content);
  
  let headingCount = 0;
  let codeBlockCount = 0;
  let linkCount = 0;
  let headings: string[] = [];
  
  visit(tree, (node) => {
    if (node.type === 'heading') {
      headingCount++;
      if (node.children?.[0]?.type === 'text') {
        headings.push(node.children[0].value.toLowerCase());
      }
    }
    if (node.type === 'code' || node.type === 'inlineCode') {
      codeBlockCount++;
    }
    if (node.type === 'link') {
      linkCount++;
    }
  });
  
  const wordCount = content.split(/\s+/).length;
  const contentLower = content.toLowerCase();
  
  // Check for key sections
  const hasInstallation = /install|setup|getting started/i.test(contentLower);
  const hasUsage = /usage|example|how to|api/i.test(contentLower);
  const hasApi = /api|interface|methods|functions/i.test(contentLower);
  const hasTesting = /test|testing|spec|jest/i.test(contentLower);
  const hasContributing = /contribut|develop|build/i.test(contentLower);
  
  // Check if generated
  const isGenerated = /generated by|placeholder|todo.*add/i.test(content);
  
  // Determine quality tier
  let qualityTier: DetailedAnalysis['analysis']['qualityTier'] = 'none';
  
  if (isGenerated || wordCount < 20) {
    qualityTier = 'placeholder';
  } else if (wordCount < 100 || headingCount < 2) {
    qualityTier = 'basic';
  } else if (wordCount > 200 && headingCount > 3 && (hasUsage || hasApi)) {
    qualityTier = 'good';
  } else if (wordCount > 400 && headingCount > 4 && hasUsage && hasApi && (hasInstallation || hasTesting)) {
    qualityTier = 'excellent';
  } else {
    qualityTier = 'basic';
  }
  
  return {
    wordCount,
    headingCount,
    codeBlockCount,
    linkCount,
    hasInstallation,
    hasUsage,
    hasApi,
    hasTesting,
    hasContributing,
    isGenerated,
    qualityTier
  };
}

async function runDetailedAnalysis() {
  const patterns = [
    'packages/**',
    'src/platform/**',
    'x-pack/packages/**',
    'x-pack/platform/packages/**',
    'x-pack/solutions/*/packages/**',
    'x-pack/solutions/*/plugins/**'
  ];
  
  const results: DetailedAnalysis[] = [];
  
  for (const pattern of patterns) {
    const paths = await glob(pattern, { onlyDirectories: true });
    
    for (const pkgPath of paths) {
      const readmePath = path.join(pkgPath, 'README.md');
      const hasReadme = fs.existsSync(readmePath);
      
      let analysis = {
        wordCount: 0,
        headingCount: 0,
        codeBlockCount: 0,
        linkCount: 0,
        hasInstallation: false,
        hasUsage: false,
        hasApi: false,
        hasTesting: false,
        hasContributing: false,
        isGenerated: false,
        qualityTier: 'none' as const
      };
      
      if (hasReadme) {
        const content = fs.readFileSync(readmePath, 'utf-8');
        analysis = analyzeMarkdown(content);
      }
      
      results.push({
        packagePath: pkgPath,
        packageType: categorizePackage(pkgPath) as any,
        hasReadme,
        analysis
      });
    }
  }
  
  // Generate comprehensive report
  const byType = results.reduce((acc, r) => {
    if (!acc[r.packageType]) acc[r.packageType] = [];
    acc[r.packageType].push(r);
    return acc;
  }, {} as Record<string, DetailedAnalysis[]>);
  
  console.log('=== Detailed Kibana Documentation Analysis ===\n');
  
  Object.entries(byType).forEach(([type, items]) => {
    console.log(`${type.toUpperCase()} (${items.length} packages):`);
    const tiers = items.reduce((acc, item) => {
      acc[item.analysis.qualityTier] = (acc[item.analysis.qualityTier] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);
    
    Object.entries(tiers).forEach(([tier, count]) => {
      console.log(`  ${tier}: ${count} (${(count/items.length*100).toFixed(1)}%)`);
    });
    console.log();
  });
  
  return results;
}

runDetailedAnalysis();
```

## Running the Analysis

✅ **All approaches are now executable:**

1. **Script-based**: `node scripts/analyze_documentation.js`
2. **Shell-based**: `./scripts/doc_analysis.sh`  
3. **AST-based**: `node scripts/advanced_doc_analysis.js`
4. **Interactive runner**: `./scripts/run_doc_analysis.sh`

Each approach offers different trade-offs:
- **Approach 1** provides balanced detail and performance
- **Approach 2** is fastest for quick metrics
- **Approach 3** offers the most comprehensive analysis

These scripts will give you statistics on documentation coverage, quality distribution, and help identify areas needing improvement across the complex Kibana codebase structure.

## Prerequisites

- Run `yarn kbn bootstrap` first (one-time setup)
- Node.js (version in `.nvmrc`)
- Standard shell tools (find, grep, wc)
